{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 14: Emerging Features and Future Directions\n",
    "\n",
    "In this tutorial, we'll explore the latest features in LangChain and LangGraph, discuss integration with other AI and ML libraries, and provide insights into keeping up with advancements in language models. We'll also cover community resources and contribution opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import Groq\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from typing import List, Union\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up the Groq LLM\n",
    "llm = Groq(api_key=os.environ[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the latest LangChain and LangGraph features\n",
    "\n",
    "Let's explore some of the newer features in LangChain, such as the `LLMSingleActionAgent` for custom agent creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom output parser\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        \n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input: (.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=lambda x: f\"Search result for {x}\",\n",
    "        description=\"Useful for searching information\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=lambda x: eval(x),\n",
    "        description=\"Useful for performing calculations\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"\n",
    "You are an AI assistant. Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "Thought: Let's approach this step-by-step:\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    partial_variables={\"tool_names\": \", \".join([tool.name for tool in tools])}\n",
    ")\n",
    "\n",
    "# LLM chain consisting of the prompt template and the LLM\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Custom agent using LLMSingleActionAgent\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=CustomOutputParser(),\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=[tool.name for tool in tools]\n",
    ")\n",
    "\n",
    "# Create an agent executor\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run the agent\n",
    "agent_executor.run(\"What is the square root of 144 plus 7?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integration with other AI and ML libraries\n",
    "\n",
    "LangChain can be integrated with various AI and ML libraries. Let's demonstrate integration with Hugging Face Transformers and TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Transformers integration\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    return f\"Sentiment: {result['label']}, Score: {result['score']:.2f}\"\n",
    "\n",
    "sentiment_tool = Tool(\n",
    "    name=\"SentimentAnalyzer\",\n",
    "    func=analyze_sentiment,\n",
    "    description=\"Analyzes the sentiment of the given text\"\n",
    ")\n",
    "\n",
    "# TensorFlow integration (simple example)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "def classify_digit(index):\n",
    "    prediction = model.predict(x_test[index:index+1])\n",
    "    return f\"The model predicts the digit is: {prediction.argmax()}\"\n",
    "\n",
    "digit_classifier = Tool(\n",
    "    name=\"DigitClassifier\",\n",
    "    func=classify_digit,\n",
    "    description=\"Classifies a handwritten digit from the MNIST dataset\"\n",
    ")\n",
    "\n",
    "# Add new tools to the agent\n",
    "tools.extend([sentiment_tool, digit_classifier])\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run the updated agent\n",
    "agent_executor.run(\"What's the sentiment of 'I love machine learning'? Also, classify the first digit in the MNIST test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keeping up with advancements in language models\n",
    "\n",
    "To stay updated with the latest advancements in language models, consider the following strategies:\n",
    "\n",
    "1. Follow research papers on arXiv, particularly in the cs.CL (Computation and Language) category.\n",
    "2. Subscribe to AI/ML newsletters like [The Batch](https://www.deeplearning.ai/the-batch/) or [Import AI](https://jack-clark.net/).\n",
    "3. Participate in AI conferences like NeurIPS, ICML, ACL, or EMNLP.\n",
    "4. Follow AI researchers and organizations on social media platforms.\n",
    "5. Experiment with new models as they become available on platforms like Hugging Face.\n",
    "\n",
    "Here's an example of how to use a newer model in LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load a newer model (e.g., GPT-Neo)\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a Hugging Face pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# Use the pipeline in LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create a simple prompt template\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's approach this step-by-step:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# Create an LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "question = \"What are the three states of matter?\"\n",
    "response = chain.run(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Community resources and contribution opportunities\n",
    "\n",
    "The LangChain and LangGraph communities are vibrant and welcoming to contributors. Here are some ways to get involved:\n",
    "\n",
    "1. **GitHub Repositories**: \n",
    "   - LangChain: [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain)\n",
    "   - LangGraph: [https://github.com/langchain-ai/langgraph](https://github.com/langchain-ai/langgraph)\n",
    "   \n",
    "   You can contribute by submitting pull requests, reporting bugs, or suggesting new features.\n",
    "\n",
    "2. **Documentation**: \n",
    "   - LangChain Docs: [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction)\n",
    "   \n",
    "   Improving documentation is always valuable. You can clarify existing docs or add examples.\n",
    "\n",
    "3. **Discord Community**: \n",
    "   - Join the LangChain Discord: [https://discord.gg/langchain](https://discord.gg/langchain)\n",
    "   \n",
    "   Engage in discussions, ask questions, and help others.\n",
    "\n",
    "4. **Twitter**: \n",
    "   - Follow [@LangChainAI](https://twitter.com/LangChainAI) for updates and announcements.\n",
    "\n",
    "5. **Blog Posts and Tutorials**: \n",
    "   - Write about your experiences with LangChain and LangGraph, create tutorials, or showcase projects.\n",
    "\n",
    "6. **LangChain Hub**: \n",
    "   - Contribute prompts, chains, and agents to the [LangChain Hub](https://github.com/hwchase17/langchain-hub).\n",
    "\n",
    "Let's create a simple example of how you might contribute a custom tool to the LangChain ecosystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    city: str = Field(..., description=\"The city to get the weather for\")\n",
    "\n",
    "class WeatherTool(BaseTool):\n",
    "    name = \"weather_tool\"\n",
    "    description = \"Useful for getting the current weather in a specified city\"\n",
    "    args_schema = WeatherInput\n",
    "    \n",
    "    def _run(self, city: str):\n",
    "        # Note: You would typically use a real API key here\n",
    "        api_key = \"your_api_key_here\"\n",
    "        base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "        \n",
    "        params = {\n",
    "            \"q\": city,\n",
    "            \"appid\": api_key,\n",
    "            \"units\": \"metric\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            weather = data[\"weather\"][0][\"description\"]\n",
    "            temp = data[\"main\"][\"temp\"]\n",
    "            return f\"The weather in {city} is {weather} with a temperature of {temp}°C\"\n",
    "        else:\n",
    "            return f\"Failed to get weather for {city}. Error: {response.status_code}\"\n",
    "    \n",
    "    async def _arun(self, city: str):\n",
    "        # For simplicity, we're using the sync version here\n",
    "        return self._run(city)\n",
    "\n",
    "# Example usage of the custom tool\n",
    "weather_tool = WeatherTool()\n",
    "result = weather_tool.run(\"London\")\n",
    "print(result)\n",
    "\n",
    "# To contribute this tool, you would typically:\n",
    "# 1. Ensure it's well-documented and follows best practices\n",
    "# 2. Write tests for the tool\n",
    "# 3. Submit a pull request to the LangChain repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored emerging features in LangChain and LangGraph, demonstrated integration with other AI and ML libraries, discussed strategies for keeping up with advancements in language models, and covered community resources and contribution opportunities.\n",
    "\n",
    "Key takeaways:\n",
    "1. LangChain and LangGraph are rapidly evolving, with new features being added regularly.\n",
    "2. These libraries can be integrated with a wide range of AI and ML tools, enhancing their capabilities.\n",
    "3. Staying updated with the latest developments in language models is crucial for leveraging their full potential.\n",
    "4. The LangChain and LangGraph communities offer numerous opportunities for learning and contributing.\n",
    "\n",
    "As you continue your journey with LangChain and LangGraph, consider the following next steps:\n",
    "\n",
    "1. Experiment with the latest features and contribute your findings back to the community.\n",
    "2. Develop and share custom tools, chains, or agents that solve specific problems.\n",
    "3. Participate in discussions on Discord or GitHub to help shape the future of these libraries.\n",
    "4. Create tutorials or blog posts to help others learn and adopt LangChain and LangGraph.\n",
    "5. Explore integrations with emerging AI technologies and share your results.\n",
    "\n",
    "Remember, the field of AI and language models is rapidly evolving. Stay curious, keep learning, and don't hesitate to contribute your unique insights and creations to the community!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
