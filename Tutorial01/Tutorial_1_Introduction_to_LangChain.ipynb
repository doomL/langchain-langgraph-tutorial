{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Introduction to LangChain\n",
    "\n",
    "Welcome to our first tutorial on LangChain! In this notebook, we'll cover the basics of LangChain and create a simple application using the Groq LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is LangChain?\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It provides a set of tools and abstractions that make it easier to build complex LLM-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation and Setup\n",
    "\n",
    "First, let's make sure we have the necessary packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the required modules and set up our Groq API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OpenAI GPT API Parameters: Temperature and Top_p Sampling\n",
    "\n",
    "1. Temperature:\n",
    "    - Controls the randomness/creativity of text generation\n",
    "    - Range: 0.0 to 1.0 (can go higher, but not recommended)\n",
    "    - Lower values (e.g., 0.2): More focused, deterministic output\n",
    "    - Higher values (e.g., 0.7): More diverse, creative output\n",
    "    - Temperature 0: Always selects the most probable next token\n",
    "    - Affects the probability distribution of possible tokens\n",
    "\n",
    "2. Top_p Sampling (Nucleus Sampling):\n",
    "    - Alternative to temperature sampling\n",
    "    - Range: 0.0 to 1.0\n",
    "    - Considers only the subset of tokens whose cumulative probability mass reaches the top_p threshold\n",
    "    - Example: top_p 0.1 considers only the top 10% most likely tokens\n",
    "    - Allows for dynamic vocabulary selection based on context\n",
    "\n",
    "3. Usage:\n",
    "    - Can be used independently or together\n",
    "    - Adjust based on desired level of creativity and control\n",
    "\n",
    "4. Extended Concepts:\n",
    "    - Perplexity: Measure of how well the model predicts the text. Lower perplexity indicates better prediction.\n",
    "    - Beam Search: Technique to find the most likely sequence of tokens by exploring multiple possibilities simultaneously.\n",
    "    - Length Penalty: Encourages or discourages the model to generate longer or shorter sequences.\n",
    "\n",
    "5. Best Practices:\n",
    "    - Experiment with different values for your specific use case\n",
    "    - Start with middle values (e.g., temperature 0.5, top_p 0.5) and adjust as needed\n",
    "    - For factual or technical tasks, use lower temperature and top_p\n",
    "    - For creative tasks, use higher temperature and top_p\n",
    "    - Consider using top_p sampling for more consistent results across different prompts\n",
    "\n",
    "6. Example Use Cases and Recommended Values:\n",
    "    Use Case                 | Temperature | Top_p | Description\n",
    "    ------------------------ | ----------- | ----- | -----------\n",
    "    Code Generation          | 0.2         | 0.1   | Focused, convention-adhering code\n",
    "    Creative Writing         | 0.7         | 0.8   | Diverse, exploratory text\n",
    "    Chatbot Responses        | 0.5         | 0.5   | Balanced coherence and diversity\n",
    "    Code Comment Generation  | 0.3         | 0.2   | Concise, relevant comments\n",
    "    Data Analysis Scripting  | 0.2         | 0.1   | Correct, efficient scripts\n",
    "    Exploratory Code Writing | 0.6         | 0.7   | Creative, alternative solutions\n",
    "\n",
    "7. Advanced Techniques:\n",
    "    - Fine-tuning: Adapt the model to specific tasks or domains for better performance\n",
    "    - Prompt Engineering: Craft effective prompts to guide the model's output\n",
    "    - Few-shot Learning: Provide examples in the prompt to steer the model's behavior\n",
    "\n",
    "8. Ethical Considerations:\n",
    "    - Be aware of potential biases in the model's output\n",
    "    - Implement content filtering for sensitive applications\n",
    "    - Respect copyright and intellectual property rights\n",
    "\n",
    "9. Performance Optimization:\n",
    "    - Use caching mechanisms to store frequently requested outputs\n",
    "    - Implement request batching for multiple related queries\n",
    "    - Consider using smaller models for faster response times in less complex tasks\n",
    "\n",
    "Remember to refer to the official OpenAI documentation for the most up-to-date information and best practices when working with the GPT API.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Set your Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.5, \"seed\": 1337}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Concepts: Chains, Agents, and Memory\n",
    "\n",
    "LangChain introduces several key concepts:\n",
    "\n",
    "- **Chains**: Sequences of operations to be performed with an LLM\n",
    "- **Agents**: Autonomous entities that can use tools and make decisions\n",
    "- **Memory**: Systems for storing and retrieving information across interactions\n",
    "\n",
    "In this tutorial, we'll focus on creating a simple chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Your First LangChain Application\n",
    "\n",
    "Let's create a simple application that generates a short story based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domenico/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'A time traveler accidentally changes history', 'text': \"As a renowned physicist, Emma had spent years perfecting her time machine. She set the dials for ancient Egypt, eager to witness the construction of the Great Pyramid. However, upon arrival, Emma's curiosity got the better of her. She approached a young pharaoh, fascinated by his architectural plans. In her enthusiasm, she accidentally knocked over a nearby scroll, altering the pyramid's original design. Returning to her own time, Emma was shocked to find the pyramid's familiar triangular shape had given way to a peculiar, curved structure. Her careless mistake had rewritten history, leaving her to ponder the consequences of her actions.\"}\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt template\n",
    "template = \"\"\"\n",
    "You are a creative writer. Write a short story (about 100 words) based on the following prompt:\n",
    "\n",
    "Prompt: {prompt}\n",
    "\n",
    "Story:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"prompt\"])\n",
    "\n",
    "# Create a chain\n",
    "story_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Generate a story\n",
    "story_prompt = \"A time traveler accidentally changes history\"\n",
    "result = story_chain.invoke(story_prompt)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'prompt': 'A time traveler accidentally changes history', \n",
    " 'text': \"As a renowned physicist, Emma had spent years perfecting her time machine. She set the dials for ancient Egypt, eager to witness the construction of the Great Pyramid. However, upon arrival, Emma's curiosity got the better of her. She approached a young pharaoh, fascinated by his architectural plans. In her enthusiasm, she accidentally knocked over a nearby scroll, altering the pyramid's original design. Returning to her own time, Emma was shocked to find the pyramid's familiar triangular shape had given way to a peculiar, curved structure. Her careless mistake had rewritten history, leaving her to ponder the consequences of her actions.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've just created your first LangChain application. This simple example demonstrates how to use a prompt template, create a chain, and generate content using an LLM.\n",
    "\n",
    "In the next tutorial, we'll explore more advanced features of LangChain and dive deeper into working with language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
