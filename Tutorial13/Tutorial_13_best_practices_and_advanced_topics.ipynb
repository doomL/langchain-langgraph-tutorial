{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Best Practices and Advanced Topics\n",
    "\n",
    "In this tutorial, we'll cover best practices and advanced topics for developing and deploying LangChain and LangGraph applications. We'll explore performance optimization, handling rate limits and API costs, security considerations, deployment strategies, and monitoring and logging in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from langchain.llms import Groq\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.cache import InMemoryCache\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Set up the Groq LLM\n",
    "llm = Groq(api_key=os.environ[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance optimization techniques\n",
    "\n",
    "Let's explore some performance optimization techniques for LangChain applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable caching\n",
    "import langchain\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# Create an async version of the LLM\n",
    "async_llm = llm.agenerate\n",
    "\n",
    "# Define a simple prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a short paragraph about {topic}.\"\n",
    ")\n",
    "\n",
    "# Create an LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Function to process topics sequentially\n",
    "def process_topics_sequential(topics: List[str]) -> List[str]:\n",
    "    return [chain.run(topic) for topic in topics]\n",
    "\n",
    "# Function to process topics asynchronously\n",
    "async def process_topics_async(topics: List[str]) -> List[str]:\n",
    "    async_chain = LLMChain(llm=async_llm, prompt=prompt)\n",
    "    tasks = [async_chain.arun(topic) for topic in topics]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "# Compare performance\n",
    "topics = [\"Python\", \"Machine Learning\", \"Artificial Intelligence\", \"Data Science\", \"Web Development\"]\n",
    "\n",
    "# Sequential processing\n",
    "%time sequential_results = process_topics_sequential(topics)\n",
    "\n",
    "# Asynchronous processing\n",
    "%time async_results = await process_topics_async(topics)\n",
    "\n",
    "print(f\"Sequential processing time: {time_sequential}\")\n",
    "print(f\"Asynchronous processing time: {time_async}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling rate limits and API costs\n",
    "\n",
    "Let's implement a system to handle rate limits and track API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Note: OpenAI is used here as an example. Adjust for Groq as needed.\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "def run_with_cost_tracking(prompt: str) -> Dict[str, Any]:\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chat_model([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.content,\n",
    "        \"total_tokens\": cb.total_tokens,\n",
    "        \"prompt_tokens\": cb.prompt_tokens,\n",
    "        \"completion_tokens\": cb.completion_tokens,\n",
    "        \"total_cost\": cb.total_cost,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = run_with_cost_tracking(\"Explain the concept of machine learning in one paragraph.\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Total tokens: {result['total_tokens']}\")\n",
    "print(f\"Total cost: ${result['total_cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Security considerations\n",
    "\n",
    "When working with LangChain and LangGraph applications, consider the following security best practices:\n",
    "\n",
    "1. Use environment variables for API keys and sensitive information\n",
    "2. Implement input validation and sanitization\n",
    "3. Use HTTPS for all API communication\n",
    "4. Implement proper authentication and authorization\n",
    "5. Regularly update dependencies\n",
    "6. Be cautious with user-provided content in prompts\n",
    "\n",
    "Here's an example of input validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "class UserInput(BaseModel):\n",
    "    prompt: str = Field(..., min_length=1, max_length=1000)\n",
    "    \n",
    "    @validator('prompt')\n",
    "    def no_sensitive_info(cls, v):\n",
    "        sensitive_words = ['password', 'credit card', 'social security']\n",
    "        if any(word in v.lower() for word in sensitive_words):\n",
    "            raise ValueError(\"Input contains sensitive information\")\n",
    "        return v\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    user_input = UserInput(prompt=\"Tell me about AI\")\n",
    "    print(\"Valid input:\", user_input)\n",
    "    \n",
    "    invalid_input = UserInput(prompt=\"My password is 123456\")\n",
    "except ValueError as e:\n",
    "    print(\"Invalid input:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying LangChain and LangGraph applications\n",
    "\n",
    "Let's create a simple FastAPI application that uses our LangChain model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate_text(query: Query):\n",
    "    try:\n",
    "        result = await chain.arun(query.text)\n",
    "        return {\"generated_text\": result}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# To run the FastAPI app, use the following command in your terminal:\n",
    "# uvicorn main:app --reload\n",
    "\n",
    "# Note: This cell won't actually start the server in the notebook.\n",
    "# It's meant to be run as a separate Python file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitoring and logging in production\n",
    "\n",
    "Let's implement basic monitoring using Prometheus metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Define Prometheus metrics\n",
    "REQUESTS = Counter('api_requests_total', 'Total API requests')\n",
    "LATENCY = Histogram('api_latency_seconds', 'API latency')\n",
    "\n",
    "# Update the FastAPI endpoint to use metrics\n",
    "@app.post(\"/generate\")\n",
    "async def generate_text(query: Query):\n",
    "    REQUESTS.inc()\n",
    "    with LATENCY.time():\n",
    "        try:\n",
    "            result = await chain.arun(query.text)\n",
    "            return {\"generated_text\": result}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Add a metrics endpoint\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    from prometheus_client import generate_latest\n",
    "    return generate_latest()\n",
    "\n",
    "# Note: This cell won't actually start the server in the notebook.\n",
    "# It's meant to be run as a separate Python file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've covered several best practices and advanced topics for LangChain and LangGraph applications:\n",
    "\n",
    "1. Performance optimization techniques, including caching and asynchronous processing\n",
    "2. Handling rate limits and tracking API costs\n",
    "3. Security considerations and input validation\n",
    "4. Deploying applications using FastAPI\n",
    "5. Monitoring and logging in production using Prometheus metrics\n",
    "\n",
    "These practices will help you build more efficient, secure, and production-ready AI applications using LangChain and LangGraph.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further improve your LangChain and LangGraph applications:\n",
    "\n",
    "1. Implement more advanced caching strategies, such as using Redis for distributed caching\n",
    "2. Explore containerization (e.g., Docker) for easier deployment and scaling\n",
    "3. Implement more comprehensive logging and error handling\n",
    "4. Set up CI/CD pipelines for automated testing and deployment\n",
    "5. Explore advanced monitoring and alerting systems\n",
    "6. Consider using a reverse proxy (e.g., Nginx) for load balancing and additional security\n",
    "7. Implement rate limiting and request throttling to protect your API\n",
    "\n",
    "Remember to always follow best practices for security, performance, and scalability as you develop and deploy your AI applications"
   ]
}
    ]
   }
