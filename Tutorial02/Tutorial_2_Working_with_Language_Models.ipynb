{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Working with Language Models in LangChain\n",
    "\n",
    "In this tutorial, we'll explore how to work with language models in LangChain, focusing on the Groq LLM. We'll cover connecting to the model, creating prompt templates, building chains, and handling responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to Language Models\n",
    "\n",
    "First, let's set up our environment and connect to the Groq LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello. How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 39, 'total_tokens': 49, 'completion_time': 0.04, 'prompt_time': 0.010833841, 'queue_time': 0.005008988999999998, 'total_time': 0.050833841}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-525ece5d-a510-42a7-8cae-bbbbdd6569ad-0'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.2, \"seed\": 1337}\n",
    "    )\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Prompt Templates\n",
    "\n",
    "Prompt templates allow us to create reusable prompts with input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question:\n",
      "You answers in the English language.\n",
      "Question: What is the capital of France?\n",
      "Answer: Let's approach this step-by-step:\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt template\n",
    "template = \"\"\"Answer the following question:\n",
    "You answers in the {language} language.\n",
    "Question: {question}\n",
    "Answer: Let's approach this step-by-step:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"language\"])\n",
    "\n",
    "# Use the prompt template\n",
    "question = \"What is the capital of France?\"\n",
    "formatted_prompt = prompt.format(question=question,language=\"English\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Simple Prompt Chains\n",
    "\n",
    "Now, let's create a chain that combines our prompt template with the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approchons cela étape par étape :\n",
      "\n",
      "La vitesse de la lumière est une constante physique fondamentale qui représente la vitesse à laquelle la lumière se propage dans le vide. Elle est notée c et est exprimée en mètres par seconde (m/s).\n",
      "\n",
      "La vitesse de la lumière est de 299 792 458 mètres par seconde. Cette valeur a été déterminée avec une grande précision grâce à des expériences et des mesures précises.\n",
      "\n",
      "Il est important de noter que la vitesse de la lumière est une constante universelle, ce qui signifie qu'elle est la même partout dans l'univers et qu'elle ne dépend pas de la vitesse de l'observateur ou de la source de lumière.\n"
     ]
    }
   ],
   "source": [
    "# Create a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"question\":\"What is the speed of light?\",\"language\":\"French\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Model Responses\n",
    "\n",
    "Let's explore different ways to handle and process model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: content='Here are three prime numbers: \\n\\n1. 7\\n2. 11\\n3. 13' response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 40, 'total_tokens': 62, 'completion_time': 0.088, 'prompt_time': 0.01033909, 'queue_time': 0.047515960999999995, 'total_time': 0.09833909}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-70984f82-3b18-4f09-a7cb-b9a4632ac4e1-0'\n",
      "\n",
      "Chain response: Pour commencer, voici quelques noms d'arbres courants en français :\n",
      "\n",
      "1. Le chêne (Oak)\n",
      "2. Le sapin (Fir)\n",
      "3. Le pin (Pine)\n",
      "4. Le cèdre (Cedar)\n",
      "5. L'érable (Maple)\n",
      "6. Le noyer (Walnut)\n",
      "7. Le châtaignier (Chestnut)\n",
      "8. Le peuplier (Poplar)\n",
      "9. Le saule (Willow)\n",
      "10. Le hêtre (Beech)\n",
      "\n",
      "Il existe de nombreux autres noms d'arbres en français, mais voici quelques-uns des plus courants. Si vous avez des questions spécifiques ou si vous souhaitez en savoir plus, n'hésitez pas à demander !\n"
     ]
    }
   ],
   "source": [
    "# Get the raw response\n",
    "raw_response = llm.invoke(\"List three prime numbers.\")\n",
    "print(\"Raw response:\", raw_response)\n",
    "\n",
    "# Using the chain with a dictionary input\n",
    "chain_response = chain.invoke({\"question\": \"tree Names\",\"language\": \"French\"})\n",
    "print(\"\\nChain response:\", chain_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "Parsed list: ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'pink', 'brown', 'grey', 'black', 'white', 'purple', 'turquoise', 'silver', 'gold', 'copper', 'bronze', 'beige', 'cream', 'peach', 'coral', 'salmon', 'lavender', 'lilac', 'magenta', 'cyan', 'teal', 'mint', 'aqua', 'navy', 'charcoal', 'ivory', 'champagne', 'plum', 'burgundy', 'crimson', 'scarlet', 'ruby', 'emerald', 'forest', 'hunter', 'olive', 'khaki', 'tan', 'taupe', 'mauve', 'fuchsia', 'periwinkle', 'sapphire', 'amethyst', 'garnet', 'onyx', 'pearl', 'moss', 'sage', 'seafoam', 'powder', 'blush', 'rose', 'blush', 'dusty rose', 'mauve', 'wisteria', 'iris', 'orchid', 'heliotrope', 'cerulean', 'cobalt', 'azure', 'steel', 'slate', 'pewter', 'rust', 'sienna', 'umber', 'sepia', 'caramel', 'honey', 'goldenrod', 'amber', 'tangerine', 'persimmon', 'pomegranate', 'mulberry', 'raspberry', 'strawberry', 'watermelon', 'cantaloupe', 'honeydew', 'lime', 'chartreuse', 'jade', 'spruce', 'fir', 'spruce', 'moss', 'lichen', 'sand', 'clay', 'terracotta', 'sienna', 'carmine', 'vermilion', 'cardinal', 'crimson', 'garnet', 'ruby', 'amaranth', 'blush', 'rosewood', 'sandalwood', 'mahogany', 'walnut', 'ebony', 'coal', 'soot', 'ash', 'smoke', 'fog', 'mist', 'slate', 'granite', 'basalt', 'obsidian', 'onyx', 'jet', 'raven', 'sable', 'inky', 'indanthrene', 'caput mortuum', 'smalt', 'ultramarine', 'Prussian blue', \"Scheele's green\", 'rose madder', 'gamboge', 'Tyrian purple', 'crimson lake', 'burnt sienna', 'raw umber', 'burnt umber', 'sepia', \"Payne's grey\", 'ivory black', 'lamp black', 'Vantablack', 'YInMn blue', 'Maya blue', 'Han purple', 'rose', 'blush', 'dusty rose', 'mauve', 'wisteria', 'iris', 'orchid', 'heliotrope', 'cerulean', 'cobalt', 'azure', 'steel', 'slate', 'pewter', 'rust', 'sienna', 'umber', 'sepia', 'caramel', 'honey', 'goldenrod', 'amber', 'tangerine', 'persimmon', 'pomegranate', 'mulberry', 'raspberry', 'strawberry', 'watermelon', 'cantaloupe', 'honeydew', 'lime', 'chartreuse', 'jade', 'spruce', 'fir', 'spruce', 'moss', 'lichen', 'sand', 'clay', 'terracotta', 'sienna', 'carmine', 'vermilion', 'cardinal', 'crimson', 'garnet', 'ruby', 'amaranth', 'blush', 'rosewood', 'sandalwood', 'mahogany', 'walnut', 'ebony', 'coal', 'soot', 'ash', 'smoke', 'fog', 'mist', 'slate', 'granite', 'basalt', 'obsidian', 'onyx', 'jet', 'raven', 'sable', 'inky', 'indanthrene', 'caput mortuum', 'smalt', 'ultramarine', 'Prussian blue', \"Scheele's green\", 'rose madder', 'gamboge', 'Tyrian purple', 'crimson lake', 'burnt sienna', 'raw umber', 'burnt umber', 'sepia', \"Payne's grey\", 'ivory black', 'lamp black', 'Vantablack', 'YInMn blue', 'Maya blue', 'Han purple']\n"
     ]
    }
   ],
   "source": [
    "# Parsing structured output\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"List 100 {item}. {format_instructions} write only the colors, nothing else\",\n",
    "    input_variables=[\"item\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = list_prompt | llm |output_parser\n",
    "result = chain.invoke({\"item\":\"colors\"})\n",
    "print(type(result))\n",
    "print(\"\\nParsed list:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Prompt Engineering\n",
    "\n",
    "Here are some best practices for effective prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------\n",
      "Specific Prompt Result:\n",
      "Quantum entanglement is a fascinating concept in physics that can be a bit tricky to understand, but I'll try to break it down in simple terms.\n",
      "\n",
      "**What is Quantum Entanglement?**\n",
      "\n",
      "Imagine you have two toy cars that are connected by a spring. If you push one car, the other car will move too, because they're connected by the spring. This is a classical example of a physical connection between two objects.\n",
      "\n",
      "Now, imagine that these toy cars are not connected by a spring, but they're still somehow \"linked\" in a way that lets them affect each other, even if they're on opposite sides of the universe. This is roughly what quantum entanglement is.\n",
      "\n",
      "**How Does it Work?**\n",
      "\n",
      "In the quantum world, particles like electrons, photons, and atoms can become \"entangled\" in a way that lets them share information with each other, even if they're separated by huge distances. When something happens to one particle, it instantly affects the other particle, no matter how far apart they are.\n",
      "\n",
      "Here's an example: imagine two entangled particles, A and B. If particle A is spinning clockwise, particle B will instantly start spinning counterclockwise, even if they're on opposite sides of the universe. This happens even if there's no physical connection between them, and even if they're separated by billions of kilometers.\n",
      "\n",
      "**The Weird Part**\n",
      "\n",
      "The really weird thing about quantum entanglement is that it happens instantly, faster than the speed of light. This is known as \"quantum non-locality.\" It's as if the information is transmitted between the particles through a kind of \"quantum internet\" that lets them communicate with each other in a way that's not limited by space or time.\n",
      "\n",
      "**What Does it Mean?**\n",
      "\n",
      "Quantum entanglement is a fundamental aspect of quantum mechanics, and it has some pretty mind-blowing implications. For example, it suggests that the universe is a more interconnected and holistic place than we thought, and that the information that makes up reality is not just local, but also non-local.\n",
      "\n",
      "It's also led to some interesting applications, such as quantum computing and quantum cryptography, which use entanglement to create secure communication channels and perform complex calculations.\n",
      "\n",
      "**In Simple Terms**\n",
      "\n",
      "To sum it up, quantum entanglement is a phenomenon where two or more particles become connected in a way that lets them affect each other, even if they're separated by huge distances. It's a fundamental aspect of quantum mechanics that has some pretty weird and wonderful implications for our understanding of the universe.\n",
      "\n",
      "---------------------------------------------------\n",
      "Few-shot Prompt Result:\n",
      "Sentiment: Neutral\n",
      "\n",
      "The text expresses a neutral sentiment as it neither strongly praises nor strongly criticizes the movie. The phrase \"I guess\" also adds a tone of uncertainty, which further supports the neutral classification.\n",
      "\n",
      "---------------------------------------------------\n",
      "Step-by-step Prompt Result:\n",
      "To solve the problem 'Calculate the area of a circle with radius 5 cm', I will follow the given steps:\n",
      "\n",
      "1. Identify the key information:\n",
      "The key information in this problem is the radius of the circle, which is 5 cm.\n",
      "\n",
      "2. Determine the appropriate formula or method:\n",
      "The formula to calculate the area of a circle is A = πr², where A is the area and r is the radius of the circle.\n",
      "\n",
      "3. Apply the formula or method step-by-step:\n",
      "Using the formula A = πr², we can substitute the value of the radius (r = 5 cm) into the formula:\n",
      "A = π(5)²\n",
      "A = π(25)\n",
      "A = 3.14159... * 25\n",
      "A ≈ 78.54\n",
      "\n",
      "4. Check your answer:\n",
      "The calculated area of the circle is approximately 78.54 square centimeters. This answer seems reasonable, as the area of a circle with a radius of 5 cm should be greater than the area of a square with a side length of 5 cm (which is 25 square centimeters).\n"
     ]
    }
   ],
   "source": [
    "# 1. Be specific and provide context\n",
    "specific_prompt = PromptTemplate(\n",
    "    template=\"You are an expert in {field}. Explain {concept} in simple terms for a beginner.\",\n",
    "    input_variables=[\"field\", \"concept\"]\n",
    ")\n",
    "\n",
    "# 2. Use examples (few-shot learning)\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"Classify the sentiment of the following text as positive, negative, or neutral.\n",
    "\n",
    "Example 1:\n",
    "Text: I love this product!\n",
    "Sentiment: Positive\n",
    "\n",
    "Example 2:\n",
    "Text: This is the worst experience ever.\n",
    "Sentiment: Negative\n",
    "\n",
    "Example 3:\n",
    "Text: The weather is cloudy today.\n",
    "Sentiment: Neutral\n",
    "\n",
    "Now, classify the following text:\n",
    "Text: {text}\n",
    "Sentiment:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# 3. Break complex tasks into steps\n",
    "step_prompt = PromptTemplate(\n",
    "    template=\"\"\"To solve the problem '{problem}', follow these steps:\n",
    "1. Identify the key information\n",
    "2. Determine the appropriate formula or method\n",
    "3. Apply the formula or method step-by-step\n",
    "4. Check your answer\n",
    "\n",
    "Now, solve the problem:\"\"\",\n",
    "    input_variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Test the prompts\n",
    "chains = {\n",
    "    \"Specific\": specific_prompt | llm,\n",
    "    \"Few-shot\": few_shot_prompt | llm,\n",
    "    \"Step-by-step\": step_prompt | llm \n",
    "    }\n",
    "\n",
    "for name, chain in chains.items():\n",
    "    print(f\"\\n---------------------------------------------------\\n{name} Prompt Result:\")\n",
    "    if name == \"Specific\":\n",
    "        print(chain.invoke({\"field\":\"physics\", \"concept\":\"quantum entanglement\"}).content)\n",
    "    elif name == \"Few-shot\":\n",
    "        print(chain.invoke({\"text\":\"This movie was okay, I guess.\"}).content)\n",
    "    else:\n",
    "        print(chain.invoke({\"problem\":\"Calculate the area of a circle with radius 5 cm\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of working with language models in LangChain, including connecting to models, creating prompt templates, building chains, handling responses, and implementing best practices for prompt engineering. These skills will serve as a foundation for building more complex applications with LangChain in future tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
