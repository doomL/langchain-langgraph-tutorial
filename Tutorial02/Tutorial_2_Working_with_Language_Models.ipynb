{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Working with Language Models in LangChain\n",
    "\n",
    "In this tutorial, we'll explore how to work with language models in LangChain, focusing on the Groq LLM. We'll cover connecting to the model, creating prompt templates, building chains, and handling responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to Language Models\n",
    "\n",
    "First, let's set up our environment and connect to the Groq LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello. How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 39, 'total_tokens': 49, 'completion_time': 0.04, 'prompt_time': 0.009920069, 'queue_time': 0.090482385, 'total_time': 0.049920069}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-fc9ba970-7365-4097-8627-22991f55302f-0'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set your Groq API key\n",
    "os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.2, \"seed\": 1337}\n",
    "    )\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Prompt Templates\n",
    "\n",
    "Prompt templates allow us to create reusable prompts with input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question:\n",
      "You answers in the English language.\n",
      "Question: What is the capital of France?\n",
      "Answer: Let's approach this step-by-step:\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt template\n",
    "template = \"\"\"Answer the following question:\n",
    "You answers in the {language} language.\n",
    "Question: {question}\n",
    "Answer: Let's approach this step-by-step:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"language\"])\n",
    "\n",
    "# Use the prompt template\n",
    "question = \"What is the capital of France?\"\n",
    "formatted_prompt = prompt.format(question=question,language=\"English\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Simple Prompt Chains\n",
    "\n",
    "Now, let's create a chain that combines our prompt template with the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vitesse de la lumière est une constante physique fondamentale qui représente la vitesse à laquelle la lumière se propage dans le vide. Pour répondre à cette question, nous allons procéder étape par étape.\n",
      "\n",
      "Étape 1 : La vitesse de la lumière est une constante universelle qui a été mesurée et calculée avec une grande précision.\n",
      "\n",
      "Étape 2 : Selon la théorie de la relativité restreinte d'Albert Einstein, la vitesse de la lumière est la même pour tous les observateurs, quelle que soit leur vitesse relative.\n",
      "\n",
      "Étape 3 : La vitesse de la lumière a été mesurée à l'aide de différentes méthodes expérimentales, notamment en utilisant des lasers et des miroirs.\n",
      "\n",
      "Étape 4 : La vitesse de la lumière a été définie comme une constante physique fondamentale par le Système international d'unités (SI) en 1983.\n",
      "\n",
      "Étape 5 : La valeur de la vitesse de la lumière est de 299 792 458 mètres par seconde (m/s).\n",
      "\n",
      "En résumé, la vitesse de la lumière est de 299 792 458 m/s.\n"
     ]
    }
   ],
   "source": [
    "# Create a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"question\":\"What is the speed of light?\",\"language\":\"French\"})\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Model Responses\n",
    "\n",
    "Let's explore different ways to handle and process model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed list: ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'pink', 'brown', 'grey', 'black', 'white', 'purple', 'turquoise', 'silver', 'gold', 'copper', 'bronze', 'beige', 'cream', 'peach', 'coral', 'salmon', 'lavender', 'lilac', 'magenta', 'cyan', 'teal', 'mint', 'aqua', 'navy', 'charcoal', 'ivory', 'champagne', 'plum', 'burgundy', 'garnet', 'ruby', 'emerald', 'sapphire', 'amethyst', 'peridot', 'topaz', 'amber', 'honey', 'mustard', 'sienna', 'umber', 'sepia', 'mahogany', 'walnut', 'ebony', 'onyx', 'jade', 'olive', 'sage', 'moss', 'fern', 'spruce', 'pine', 'cedar', 'sand', 'clay', 'terracotta', 'sienna', 'carmine', 'crimson', 'scarlet', 'vermilion', 'cerulean', 'cobalt', 'azure', 'steel', 'pewter', 'rose', 'blush', 'powder', 'mauve', 'wisteria', 'iris', 'lapis', 'opal', 'pearl', 'taupe', 'mocha', 'caramel', 'tawny', 'goldenrod', 'lemon', 'lime', 'chartreuse', 'celadon', 'seafoam', 'mist', 'fog', 'smoke', 'ash', 'slate', 'granite', 'basalt', 'obsidian', 'coal', 'ink', 'raven', 'sable', 'ebony', 'jet', 'beryl', 'tourmaline', 'spinel', 'garnet', 'citrine', 'ammolite', 'sunstone', 'moonstone', 'opalescent', 'iridescent', 'mother-of-pearl', 'rosewood', 'sandalwood', 'teak', 'walnut', 'chestnut', 'hazelnut', 'pecan', 'caramel', 'toffee', 'butterscotch', 'honeycomb', 'golden', 'apricot', 'nectarine', 'peach', 'cantaloupe', 'honeydew', 'mint', 'pistachio', 'sage', 'seafoam', 'lavender', 'lilac', 'wisteria', 'orchid', 'violet', 'fuchsia', 'magenta', 'raspberry', 'strawberry', 'watermelon', 'blueberry', 'boysenberry', 'mulberry', 'plum', 'pomegranate', 'cranberry', 'cherry', 'burgundy', 'merlot', 'crimson', 'scarlet', 'vermilion', 'cardinal', 'garnet', 'ruby', 'emerald', 'sapphire', 'amethyst', 'peridot', 'topaz', 'turquoise', 'aquamarine', 'opal', 'pearl', 'onyx', 'jade', 'obsidian', 'coal', 'slate', 'granite', 'basalt', 'sand', 'clay', 'terracotta', 'sienna', 'umber', 'sepia', 'mahogany', 'walnut', 'ebony', 'ivory', 'champagne', 'beige', 'cream', 'peach', 'coral', 'salmon', 'pink', 'rose', 'blush', 'powder', 'mauve', 'wisteria', 'iris', 'lapis', 'opal', 'pearl', 'taupe', 'mocha', 'caramel', 'tawny', 'goldenrod', 'lemon', 'lime', 'chartreuse', 'celadon', 'seafoam', 'mist', 'fog', 'smoke', 'ash', 'slate', 'granite', 'basalt', 'obsidian', 'coal', 'ink', 'raven', 'sable', 'ebony', 'jet', 'beryl', 'tourmaline', 'spinel', 'garnet', 'citrine', 'ammolite', 'sunstone', 'moonstone', 'opalescent', 'iridescent', 'mother-of-pearl']\n"
     ]
    }
   ],
   "source": [
    "# # Get the raw response\n",
    "# raw_response = llm.invoke(\"List three prime numbers.\")\n",
    "# print(\"Raw response:\", raw_response)\n",
    "\n",
    "# # Using the chain with a dictionary input\n",
    "# chain_response = chain({\"item\": \"tree Names\",\"language\": \"French\"})\n",
    "# print(\"\\nChain response:\", chain_response['text'])\n",
    "\n",
    "# Parsing structured output\n",
    "\n",
    "\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"List 100 {item}. {format_instructions} write only the colors, nothing else\",\n",
    "    input_variables=[\"item\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=list_prompt)\n",
    "result = chain.run(item=\"colors\")\n",
    "parsed_result = output_parser.parse(result)\n",
    "print(\"\\nParsed list:\", parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Prompt Engineering\n",
    "\n",
    "Here are some best practices for effective prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Specific Prompt Result:\n",
      "\n",
      "Few-shot Prompt Result:\n",
      "\n",
      "Step-by-step Prompt Result:\n",
      "To solve the problem 'Calculate the area of a circle with radius 5 cm', I will follow the given steps:\n",
      "\n",
      "1. Identify the key information:\n",
      "The key information in this problem is the radius of the circle, which is 5 cm.\n",
      "\n",
      "2. Determine the appropriate formula or method:\n",
      "The formula to calculate the area of a circle is A = πr², where A is the area and r is the radius of the circle.\n",
      "\n",
      "3. Apply the formula or method step-by-step:\n",
      "Using the formula A = πr², we can substitute the value of the radius (r = 5 cm) into the formula:\n",
      "A = π(5)²\n",
      "A = π(25)\n",
      "A = 3.14159... * 25\n",
      "A ≈ 78.54\n",
      "\n",
      "4. Check your answer:\n",
      "The calculated area of the circle is approximately 78.54 square centimeters. This answer seems reasonable, as the area of a circle with a radius of 5 cm should be greater than the area of a square with a side length of 5 cm (which is 25 square centimeters).\n"
     ]
    }
   ],
   "source": [
    "# 1. Be specific and provide context\n",
    "specific_prompt = PromptTemplate(\n",
    "    template=\"You are an expert in {field}. Explain {concept} in simple terms for a beginner.\",\n",
    "    input_variables=[\"field\", \"concept\"]\n",
    ")\n",
    "\n",
    "# 2. Use examples (few-shot learning)\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"Classify the sentiment of the following text as positive, negative, or neutral.\n",
    "\n",
    "Example 1:\n",
    "Text: I love this product!\n",
    "Sentiment: Positive\n",
    "\n",
    "Example 2:\n",
    "Text: This is the worst experience ever.\n",
    "Sentiment: Negative\n",
    "\n",
    "Example 3:\n",
    "Text: The weather is cloudy today.\n",
    "Sentiment: Neutral\n",
    "\n",
    "Now, classify the following text:\n",
    "Text: {text}\n",
    "Sentiment:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# 3. Break complex tasks into steps\n",
    "step_prompt = PromptTemplate(\n",
    "    template=\"\"\"To solve the problem '{problem}', follow these steps:\n",
    "1. Identify the key information\n",
    "2. Determine the appropriate formula or method\n",
    "3. Apply the formula or method step-by-step\n",
    "4. Check your answer\n",
    "\n",
    "Now, solve the problem:\"\"\",\n",
    "    input_variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Test the prompts\n",
    "chains = {\n",
    "    \"Specific\": LLMChain(llm=llm, prompt=specific_prompt),\n",
    "    \"Few-shot\": LLMChain(llm=llm, prompt=few_shot_prompt),\n",
    "    \"Step-by-step\": LLMChain(llm=llm, prompt=step_prompt)\n",
    "}\n",
    "\n",
    "for name, chain in chains.items():\n",
    "    print(f\"\\n{name} Prompt Result:\")\n",
    "    if name == \"Specific\":\n",
    "        chain.run(field=\"physics\", concept=\"quantum entanglement\")\n",
    "    elif name == \"Few-shot\":\n",
    "        chain.run(text=\"This movie was okay, I guess.\")\n",
    "    else:\n",
    "        print(chain.run(problem=\"Calculate the area of a circle with radius 5 cm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of working with language models in LangChain, including connecting to models, creating prompt templates, building chains, handling responses, and implementing best practices for prompt engineering. These skills will serve as a foundation for building more complex applications with LangChain in future tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
