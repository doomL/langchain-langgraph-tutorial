{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Working with Language Models in LangChain\n",
    "\n",
    "In this tutorial, we'll explore how to work with language models in LangChain, focusing on the Groq LLM. We'll cover connecting to the model, creating prompt templates, building chains, and handling responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to Language Models\n",
    "\n",
    "First, let's set up our environment and connect to the Groq LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello. How can I assist you today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 39, 'total_tokens': 49, 'completion_time': 0.04, 'prompt_time': 0.007960822, 'queue_time': 0.007323866, 'total_time': 0.047960822}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_9260b4bb2e', 'finish_reason': 'stop', 'logprobs': None} id='run-50f5fde5-d204-4d16-8f89-ea390cdfb2f7-0' usage_metadata={'input_tokens': 39, 'output_tokens': 10, 'total_tokens': 49}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"top_p\": 0.2, \"seed\": 1337}\n",
    "    )\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Prompt Templates\n",
    "\n",
    "Prompt templates allow us to create reusable prompts with input variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question:\n",
      "You answers in the English language.\n",
      "Question: What is the capital of France?\n",
      "Answer: Let's approach this step-by-step:\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt template\n",
    "template = \"\"\"Answer the following question:\n",
    "You answers in the {language} language.\n",
    "Question: {question}\n",
    "Answer: Let's approach this step-by-step:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"language\"])\n",
    "\n",
    "# Use the prompt template\n",
    "question = \"What is the capital of France?\"\n",
    "formatted_prompt = prompt.format(question=question,language=\"English\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Simple Prompt Chains\n",
    "\n",
    "Now, let's create a chain that combines our prompt template with the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La vitesse de la lumière est une constante fondamentale en physique. Elle est notée c et est égale à environ 299 792 458 mètres par seconde. Cette valeur est une constante universelle et ne dépend pas de la vitesse de l'observateur ou de la source de lumière. Elle est une des constantes les plus précises et les plus importantes en physique, et elle joue un rôle crucial dans de nombreuses théories, notamment la relativité restreinte et la relativité générale d'Einstein.\n"
     ]
    }
   ],
   "source": [
    "# Create a chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"question\":\"What is the speed of light?\",\"language\":\"French\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Model Responses\n",
    "\n",
    "Let's explore different ways to handle and process model responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: content='Here are three prime numbers:\\n\\n1. 7\\n2. 11\\n3. 23' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 40, 'total_tokens': 61, 'completion_time': 0.084, 'prompt_time': 0.007817887, 'queue_time': 0.038824739999999996, 'total_time': 0.091817887}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-eedd7592-b619-4f72-840c-726811bd82db-0' usage_metadata={'input_tokens': 40, 'output_tokens': 21, 'total_tokens': 61}\n",
      "\n",
      "Chain response: Pour commencer, voici quelques noms d'arbres courants en français :\n",
      "\n",
      "1. Arbre fruitier : Pommier (Pomme), Pêcher (Pêche), Cerisier (Cerise)\n",
      "2. Arbre à feuilles : Chêne, Hêtre, Érable\n",
      "3. Arbre à aiguilles : Pin, Sapin, Épicéa\n",
      "4. Arbre tropical : Palmier, Cocotier, Manglier\n",
      "5. Arbre ornemental : Lilas, Fleur d'oranger, Magnolia\n",
      "\n",
      "Voici quelques autres exemples :\n",
      "\n",
      "* Le chêne est un arbre robuste et durable.\n",
      "* Le pommier est un arbre fruitier très répandu.\n",
      "* Le pin est un arbre à aiguilles qui pousse dans les régions montagneuses.\n",
      "* Le palmier est un arbre tropical qui pousse dans les régions chaudes.\n",
      "\n",
      "J'espère que cela vous aidera !\n"
     ]
    }
   ],
   "source": [
    "# Get the raw response\n",
    "raw_response = llm.invoke(\"List three prime numbers.\")\n",
    "print(\"Raw response:\", raw_response)\n",
    "\n",
    "# Using the chain with a dictionary input\n",
    "chain_response = chain.invoke({\"question\": \"tree Names\",\"language\": \"French\"})\n",
    "print(\"\\nChain response:\", chain_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "Parsed list: ['Red', 'Orange', 'Yellow', 'Green', 'Blue', 'Indigo', 'Violet', 'Pink', 'Brown', 'Grey', 'Black', 'White', 'Turquoise', 'Silver', 'Gold', 'Copper', 'Bronze', 'Beige', 'Cream', 'Ivory', 'Lavender', 'Peach', 'Magenta', 'Cyan', 'Teal', 'Coral', 'Salmon', 'Maroon', 'Navy', 'Royal Blue', 'Baby Blue', 'Powder Blue', 'Periwinkle', 'Mint', 'Sage', 'Seafoam', 'Lilac', 'Plum', 'Burgundy', 'Charcoal', 'Taupe', 'Mocha', 'Sienna', 'Umber', 'Ochre', 'Sepia', 'Onyx', 'Ruby', 'Emerald', 'Sapphire', 'Amethyst', 'Garnet', 'Topaz', 'Opal', 'Jade', 'Pearl', 'Ivory', 'Champagne', 'Blush', 'Fuchsia', 'Cerulean', 'Azure', 'Cobalt', 'Steel', 'Chrome', 'Nickel', 'Bronze', 'Brass', 'Mahogany', 'Walnut', 'Ebony', 'Ash', 'Spruce', 'Fir', 'Sagebrush', 'Sand', 'Clay', 'Terracotta', 'Sienna', 'Carmine', 'Crimson', 'Scarlet', 'Vermilion', 'Coral', 'Salmon', 'Tangerine', 'Persimmon', 'Carrot', 'Saffron', 'Mustard', 'Olive', 'Sage', 'Moss', 'Fern', 'Spruce', 'Eucalyptus', 'Mint', 'Basil', 'Thyme', 'Rosemary', 'Sage', 'Seafoam', 'Driftwood', 'Weathered Wood', 'Leather', 'Cognac', 'Whiskey', 'Mocha', 'Taupe', 'Chamois', 'Camel', 'Beige', 'Cream', 'Ivory', 'Linen', 'Snow', 'Frost', 'Glacier', 'Iceberg', 'Permafrost', 'Slate', 'Granite', 'Basalt', 'Obsidian', 'Charcoal', 'Ink', 'Licorice', 'Espresso', 'Chocolate', 'Mocha', 'Caramel', 'Honey', 'Goldenrod', 'Amber', 'Tawny', 'Sienna', 'Umber', 'Ochre', 'Clay', 'Terracotta', 'Coral', 'Salmon', 'Shrimp', 'Peach', 'Apricot', 'Cantaloupe', 'Honeydew', 'Mint', 'Sage', 'Seafoam', 'Aquamarine', 'Turquoise', 'Lapis', 'Sapphire', 'Amethyst', 'Garnet', 'Ruby', 'Emerald', 'Jade', 'Pearl', 'Opal', 'Onyx', 'Ivory', 'Cream', 'Beige', 'Linen', 'Snow', 'Frost', 'Glacier', 'Iceberg', 'Permafrost.']\n"
     ]
    }
   ],
   "source": [
    "# Parsing structured output\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"List 100 {item}. {format_instructions} write only the colors, nothing else\",\n",
    "    input_variables=[\"item\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chain = list_prompt | llm |output_parser\n",
    "result = chain.invoke({\"item\":\"colors\"})\n",
    "print(type(result))\n",
    "print(\"\\nParsed list:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices for Prompt Engineering\n",
    "\n",
    "Here are some best practices for effective prompt engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------\n",
      "Specific Prompt Result:\n",
      "Quantum entanglement is a fascinating concept in physics that can be a bit tricky to grasp, but I'll try to explain it in simple terms.\n",
      "\n",
      "**What is Quantum Entanglement?**\n",
      "\n",
      "Imagine you have two toy cars that are connected by a spring. If you push one car, the other car will move too, because they're connected by the spring. This is a classical example of how two objects can be connected and affect each other.\n",
      "\n",
      "Now, imagine that these toy cars are not connected by a spring, but they're still somehow \"linked\" in a way that lets them affect each other, even if they're on opposite sides of the universe. This is roughly what quantum entanglement is.\n",
      "\n",
      "**How Does it Work?**\n",
      "\n",
      "In the quantum world, tiny particles like atoms and electrons can become \"entangled\" in a way that lets them affect each other, even if they're separated by huge distances. When two particles are entangled, their properties (like spin, energy, or momentum) become connected in a way that can't be explained by classical physics.\n",
      "\n",
      "Here's an example: imagine two entangled particles, A and B. If particle A has a certain spin (like \"up\" or \"down\"), particle B will instantly have the opposite spin, even if they're separated by billions of kilometers. This happens even if no physical force or signal is sent between them.\n",
      "\n",
      "**The Weird Part**\n",
      "\n",
      "The really weird thing about quantum entanglement is that it happens instantly, no matter how far apart the particles are. This means that if something happens to particle A, it instantly affects particle B, even if they're on opposite sides of the universe. This is faster than the speed of light, which is the fastest speed at which information can travel in the classical world.\n",
      "\n",
      "**What Does it Mean?**\n",
      "\n",
      "Quantum entanglement is a fundamental aspect of quantum mechanics, and it has been experimentally confirmed many times. It shows that the quantum world is a strange and non-intuitive place, where particles can be connected in ways that defy classical understanding.\n",
      "\n",
      "Entanglement has many potential applications, including quantum computing, cryptography, and even quantum teleportation (which is a way of transferring information from one particle to another without physical transport of the particles themselves).\n",
      "\n",
      "I hope this explanation helps you understand quantum entanglement in simple terms!\n",
      "\n",
      "---------------------------------------------------\n",
      "Few-shot Prompt Result:\n",
      "Sentiment: Neutral\n",
      "\n",
      "The text expresses a neutral sentiment as it neither strongly praises nor strongly criticizes the movie. The phrase \"I guess\" also adds to the ambiguity, indicating a lack of strong emotions or opinions.\n",
      "\n",
      "---------------------------------------------------\n",
      "Step-by-step Prompt Result:\n",
      "To solve the problem 'Calculate the area of a circle with radius 5 cm', I will follow the given steps:\n",
      "\n",
      "1. Identify the key information:\n",
      "The key information in this problem is the radius of the circle, which is 5 cm.\n",
      "\n",
      "2. Determine the appropriate formula or method:\n",
      "The formula to calculate the area of a circle is A = πr², where A is the area and r is the radius of the circle.\n",
      "\n",
      "3. Apply the formula or method step-by-step:\n",
      "Using the formula A = πr², we can substitute the given radius (r = 5 cm) into the formula:\n",
      "A = π(5)²\n",
      "A = π(25)\n",
      "A = 3.14159... * 25\n",
      "A ≈ 78.54\n",
      "\n",
      "4. Check your answer:\n",
      "The calculated area of the circle is approximately 78.54 square centimeters. This answer seems reasonable, as the area of a circle with a radius of 5 cm should be greater than the area of a square with a side length of 5 cm (which is 25 square centimeters).\n"
     ]
    }
   ],
   "source": [
    "# 1. Be specific and provide context\n",
    "specific_prompt = PromptTemplate(\n",
    "    template=\"You are an expert in {field}. Explain {concept} in simple terms for a beginner.\",\n",
    "    input_variables=[\"field\", \"concept\"]\n",
    ")\n",
    "\n",
    "# 2. Use examples (few-shot learning)\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"Classify the sentiment of the following text as positive, negative, or neutral.\n",
    "\n",
    "Example 1:\n",
    "Text: I love this product!\n",
    "Sentiment: Positive\n",
    "\n",
    "Example 2:\n",
    "Text: This is the worst experience ever.\n",
    "Sentiment: Negative\n",
    "\n",
    "Example 3:\n",
    "Text: The weather is cloudy today.\n",
    "Sentiment: Neutral\n",
    "\n",
    "Now, classify the following text:\n",
    "Text: {text}\n",
    "Sentiment:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# 3. Break complex tasks into steps\n",
    "step_prompt = PromptTemplate(\n",
    "    template=\"\"\"To solve the problem '{problem}', follow these steps:\n",
    "1. Identify the key information\n",
    "2. Determine the appropriate formula or method\n",
    "3. Apply the formula or method step-by-step\n",
    "4. Check your answer\n",
    "\n",
    "Now, solve the problem:\"\"\",\n",
    "    input_variables=[\"problem\"]\n",
    ")\n",
    "\n",
    "# Test the prompts\n",
    "chains = {\n",
    "    \"Specific\": specific_prompt | llm,\n",
    "    \"Few-shot\": few_shot_prompt | llm,\n",
    "    \"Step-by-step\": step_prompt | llm \n",
    "    }\n",
    "\n",
    "for name, chain in chains.items():\n",
    "    print(f\"\\n---------------------------------------------------\\n{name} Prompt Result:\")\n",
    "    if name == \"Specific\":\n",
    "        print(chain.invoke({\"field\":\"physics\", \"concept\":\"quantum entanglement\"}).content)\n",
    "    elif name == \"Few-shot\":\n",
    "        print(chain.invoke({\"text\":\"This movie was okay, I guess.\"}).content)\n",
    "    else:\n",
    "        print(chain.invoke({\"problem\":\"Calculate the area of a circle with radius 5 cm\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of working with language models in LangChain, including connecting to models, creating prompt templates, building chains, handling responses, and implementing best practices for prompt engineering. These skills will serve as a foundation for building more complex applications with LangChain in future tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
