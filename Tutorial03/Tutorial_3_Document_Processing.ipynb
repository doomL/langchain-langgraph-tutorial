{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Document Processing with LangChain\n",
    "\n",
    "In this tutorial, we'll explore document processing techniques using LangChain. We'll cover loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS,Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq LLM\n",
    "llm =  ChatGroq(\n",
    "        model_name=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.7,\n",
    "        model_kwargs={\"top_p\": 0.8, \"seed\": 1337}\n",
    "    )\n",
    "print(os.getenv('OLLAMA_EMBEDDING_URL'))\n",
    "embedding_model = OllamaEmbeddings(model=\"all-minilm\",base_url=os.getenv('OLLAMA_EMBEDDING_URL'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Parsing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of sample1.txt:\n",
      "# Comprehensive Overview of Artificial Intelligence\n",
      "\n",
      "## Table of Contents\n",
      "1. [Introduction to Artificial Intelligence](#introduction-to-artificial-intelligence)\n",
      "2. [History of AI](#history-of-ai)\n",
      "3. [...\n",
      "\n",
      "Number of documents loaded: 1\n",
      "Document 1 preview: # Comprehensive Overview of Artificial Intelligenc...\n"
     ]
    }
   ],
   "source": [
    "# Load a single document\n",
    "loader = TextLoader(\"sample_documents/sample1.txt\")\n",
    "document = loader.load()\n",
    "\n",
    "print(f\"Content of sample1.txt:\\n{document[0].page_content[:200]}...\\n\")\n",
    "\n",
    "# Load multiple documents from a directory\n",
    "dir_loader = DirectoryLoader(\"sample_documents/\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "documents = dir_loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1} preview: {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Carica il PDF\n",
    "loader = PyPDFLoader(\"sample_documents/sample2.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Splitting and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits: 111\n",
      "First split preview:\n",
      "Quiet-STaR: Language Models Can Teach Themselves to\n",
      "Think Before Speaking\n",
      "Eric Zelikman\n",
      "Stanford UniversityGeorges Harik\n",
      "Notbad AI IncYijia Shao\n",
      "Stanford UniversityVaruna Jayasiri\n",
      "Notbad AI Inc\n",
      "Nick H...\n"
     ]
    }
   ],
   "source": [
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Number of splits: {len(splits)}\")\n",
    "print(f\"First split preview:\\n{splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Question-Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main topic of these documents?\n",
      "Answer: The main topic of these documents appears to be language models (LMs) and their improvement, specifically in the areas of reasoning, answering difficult questions, and understanding text.\n",
      "\n",
      "Sources:\n",
      "Document 1: Vaishnavh Nagarajan. Think before you speak: Training language models with pause\n",
      "tokens. arXiv prepr...\n",
      "Document 2: improve the LMâ€™s ability to directly answer difficult questions. In particular,\n",
      "after continued pret...\n",
      "Document 3: these tends to<|startthought|> in some sense - to be the more difficult<|\n",
      "endthought|> trickiest for...\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(splits, embedding_model)\n",
    "\n",
    "# Create a retrieval-based QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is the main topic of these documents?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result['result']}\\n\")\n",
    "print(\"Sources:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"Document {i+1}: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Discuss the importance of AI\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "Result 1:\n",
      "expensive, difficult to scale, and provides no clear path to solving problems harder than\n",
      "those that the annotators are capable of solving.\n",
      "Another direction for teaching reasoning relies on a languag...\n",
      "\n",
      "Result 2:\n",
      "process-and outcome-based feedback. Neural Information Processing Systems (NeurIPS\n",
      "2022) Workshop on MATH-AI , 2022.\n",
      "Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D\n",
      "Good...\n",
      "\n",
      "Result 3:\n",
      "Quiet-STaR: Language Models Can Teach Themselves to\n",
      "Think Before Speaking\n",
      "Eric Zelikman\n",
      "Stanford UniversityGeorges Harik\n",
      "Notbad AI IncYijia Shao\n",
      "Stanford UniversityVaruna Jayasiri\n",
      "Notbad AI Inc\n",
      "Nick H...\n",
      "\n",
      "Question: What are some advantages of ai models?\n",
      "Answer: content='Some advantages of AI models, specifically language models, mentioned in the context are:\\n\\n1. Ability to learn from their own generated reasoning, through self-play methods such as process-and outcome-based feedback.\\n2. Capability to infer unstated rationales in arbitrary text, allowing them to learn and reason without being constrained to specific question-answering tasks.\\n3. Ability to teach themselves to think before speaking, through methods such as the Self-Taught Reasoner (STaR) and Quiet-STaR.\\n4. Potential to overcome the limitations of human annotators, such as being expensive, difficult to scale, and limited in their ability to solve problems.\\n\\nThese advantages suggest that AI models, particularly language models, have the potential to learn and reason in a more autonomous and flexible way, without being limited by human constraints.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 518, 'total_tokens': 684, 'completion_time': 0.664, 'prompt_time': 0.121612051, 'queue_time': 0.0053258079999999874, 'total_time': 0.785612051}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_5c5d1b5cfb', 'finish_reason': 'stop', 'logprobs': None} id='run-7cec066a-6091-4f4a-a39a-fc8d248c8bb2-0' usage_metadata={'input_tokens': 518, 'output_tokens': 166, 'total_tokens': 684}\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search\n",
    "query = \"Discuss the importance of AI\"\n",
    "search_results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Search query: {query}\\n\")\n",
    "print(\"Top 3 relevant chunks:\")\n",
    "for i, doc in enumerate(search_results):\n",
    "    print(f\"Result {i+1}:\\n{doc.page_content[:200]}...\\n\")\n",
    "\n",
    "# Use the search results to answer a question\n",
    "question = \"What are some advantages of ai models?\"\n",
    "context = \"\\n\".join([doc.page_content for doc in search_results])\n",
    "\n",
    "prompt = f\"Based on the following context, answer the question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored various aspects of document processing with LangChain, including loading and parsing documents, text splitting, building a simple question-answering system, and implementing semantic search. These techniques form the foundation for more advanced document analysis and information retrieval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
